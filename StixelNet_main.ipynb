{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4154783090861678259\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11272650752\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15396767698370429213\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess_func import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3229, 370, 24, 3)\n",
      "(3229,)\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "X,y = preprocess_filtering_data(date='2011_09_26', out_name='train_2', serieses=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H=370\n",
    "W=24\n",
    "C=3\n",
    "img_size_flat = H*W\n",
    "img_shape = (H,W, C)\n",
    "num_classes = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hot = np.zeros((y.shape[0], 47))\n",
    "y_hot[np.arange(y.shape[0]), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3229, 370, 24, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=X.reshape(-1,img_size_flat,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3229, 8880, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[100:]\n",
    "y_train = y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X[:100]\n",
    "y_test = y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(X_train)},\n",
    "    y=np.array(y_train),\n",
    "    batch_size=2,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.estimator.inputs.numpy_io.numpy_input_fn.<locals>.input_fn>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'x': <tf.Tensor 'random_shuffle_queue_DequeueMany_2:1' shape=(16, 8880, 3) dtype=float32>},\n",
       " <tf.Tensor 'random_shuffle_queue_DequeueMany_2:2' shape=(16,) dtype=int64>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(X_test)},\n",
    "    y=np.array(y_test),\n",
    "    num_epochs=1,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\":np.array(X_test[:20])},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # Args:\n",
    "    #\n",
    "    # features: This is the x-arg from the input_fn.\n",
    "    # labels:   This is the y-arg from the input_fn,\n",
    "    #           see e.g. train_input_fn for these two.\n",
    "    # mode:     Either TRAIN, EVAL, or PREDICT\n",
    "    # params:   User-defined hyper-parameters, e.g. learning-rate.\n",
    "    \n",
    "    # Reference to the tensor named \"x\" in the input-function.\n",
    "    x = features[\"x\"]\n",
    "\n",
    "    # The convolutional layers expect 4-rank tensors\n",
    "    # but x is a 2-rank tensor, so reshape it.\n",
    "    net = tf.reshape(x, [-1, W, H, C])    \n",
    "\n",
    "    # First convolutional layer.\n",
    "    net = tf.layers.conv2d(inputs=net, name='layer_conv1',\n",
    "                           filters=64, kernel_size=(11,5),\n",
    "                           padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=(8,4), strides=1)\n",
    "    #net = tf.nn.lrn(input=net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "    # Second convolutional layer.\n",
    "    net = tf.layers.conv2d(inputs=net, name='layer_conv2',\n",
    "                           filters=200, kernel_size=(5,3),\n",
    "                           padding='same', activation=tf.nn.relu) #200\n",
    "    #net = tf.nn.lrn(input=net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=(4,3), strides=1)    \n",
    "\n",
    "    # Flatten to a 2-rank tensor.\n",
    "    #net = tf.contrib.layers.flatten(net)\n",
    "    # Eventually this should be replaced with:\n",
    "    net = tf.layers.flatten(net)\n",
    "\n",
    "    # First fully-connected / dense layer.\n",
    "    # This uses the ReLU activation function.\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc1',\n",
    "                          units=206, activation=tf.nn.relu)     #1024\n",
    "    \n",
    "    # Second fully-connected / dense layer\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc2',\n",
    "                          units=206, activation=tf.nn.relu)   #2048\n",
    "    \n",
    "   \n",
    "    # This is the last layer so it does not use an activation function.\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc3',\n",
    "                          units=47)\n",
    "\n",
    "    # Logits output of the neural network.\n",
    "    logits = net\n",
    "\n",
    "    # Softmax output of the neural network.\n",
    "    y_pred = tf.nn.softmax(logits=logits)\n",
    "    \n",
    "    # Classification output of the neural network.\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If the estimator is supposed to be in prediction-mode\n",
    "        # then use the predicted class-number that is output by\n",
    "        # the neural network. Optimization etc. is not needed.\n",
    "        spec = tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=y_pred_cls)\n",
    "    else:\n",
    "        # Otherwise the estimator is supposed to be in either\n",
    "        # training or evaluation-mode. Note that the loss-function\n",
    "        # is also required in Evaluation mode.\n",
    "        \n",
    "        # Define the loss-function to be optimized, by first\n",
    "        # calculating the cross-entropy between the output of\n",
    "        # the neural network and the true labels for the input data.\n",
    "        # This gives the cross-entropy for each image in the batch.\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                       logits=logits)\n",
    "\n",
    "        # Reduce the cross-entropy batch-tensor to a single number\n",
    "        # which can be used in optimization of the neural network.\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        # Define the optimizer for improving the neural network.\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "\n",
    "        # Get the TensorFlow op for doing a single optimization step.\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Define the evaluation metrics,\n",
    "        # in this case the classification accuracy.\n",
    "        metrics = \\\n",
    "        {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels, y_pred_cls)\n",
    "        }\n",
    "\n",
    "        # Wrap all of this in an EstimatorSpec.\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=metrics)\n",
    "        \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"learning_rate\": 1e-4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_service': None, '_global_id_in_cluster': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7ec5a03828>, '_num_ps_replicas': 0, '_session_config': None, '_task_type': 'worker', '_model_dir': './checkpoints_tutorial17-4/', '_save_summary_steps': 100, '_log_step_count_steps': 100, '_save_checkpoints_steps': None, '_master': '', '_evaluation_master': '', '_is_chief': True, '_save_checkpoints_secs': 600, '_train_distribute': None, '_keep_checkpoint_max': 5, '_num_worker_replicas': 1, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_task_id': 0}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                               params=params,\n",
    "                               model_dir=\"./checkpoints_tutorial17-4/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./checkpoints_tutorial17-4/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.8340113, step = 0\n",
      "INFO:tensorflow:global_step/sec: 6.81094\n",
      "INFO:tensorflow:loss = 3.2863345, step = 100 (14.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.86673\n",
      "INFO:tensorflow:loss = 2.2262, step = 200 (14.563 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.82246\n",
      "INFO:tensorflow:loss = 2.656669, step = 300 (14.657 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.82563\n",
      "INFO:tensorflow:loss = 3.4271998, step = 400 (14.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81736\n",
      "INFO:tensorflow:loss = 2.6994343, step = 500 (14.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.82244\n",
      "INFO:tensorflow:loss = 1.5745263, step = 600 (14.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81245\n",
      "INFO:tensorflow:loss = 1.773667, step = 700 (14.679 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.82647\n",
      "INFO:tensorflow:loss = 1.9292066, step = 800 (14.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.82464\n",
      "INFO:tensorflow:loss = 3.5241125, step = 900 (14.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81019\n",
      "INFO:tensorflow:loss = 1.3508769, step = 1000 (14.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81087\n",
      "INFO:tensorflow:loss = 2.230201, step = 1100 (14.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81516\n",
      "INFO:tensorflow:loss = 2.2860637, step = 1200 (14.673 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81478\n",
      "INFO:tensorflow:loss = 2.1566238, step = 1300 (14.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.8222\n",
      "INFO:tensorflow:loss = 1.0376593, step = 1400 (14.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.82185\n",
      "INFO:tensorflow:loss = 2.1507275, step = 1500 (14.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81825\n",
      "INFO:tensorflow:loss = 1.1765926, step = 1600 (14.667 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.822\n",
      "INFO:tensorflow:loss = 2.919223, step = 1700 (14.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.82187\n",
      "INFO:tensorflow:loss = 2.6223593, step = 1800 (14.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.81983\n",
      "INFO:tensorflow:loss = 1.3842955, step = 1900 (14.663 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into ./checkpoints_tutorial17-4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.5179362.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f7ec5a03f28>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=train_input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-06-17:40:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints_tutorial17-4/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-06-17:40:42\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.42, global_step = 2000, loss = 1.6558238\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.42, 'global_step': 2000, 'loss': 1.6558238}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 42.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification accuracy: {0:.2%}\".format(result[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints_tutorial17-4/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([19, 46, 29, 28, 24, 24, 28, 26, 22, 29, 28, 22, 24, 19, 18, 29, 28,\n",
       "       26, 28, 29])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred = np.array(list(predictions))\n",
    "cls_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 28, 29, 27, 25, 23, 18, 26, 22, 29, 28, 22, 24, 14, 17, 28, 28,\n",
       "       25, 27, 29])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "- split to train and validation in a smarter way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(num_training=2000, num_validation=500, num_test=10000, date='2011_09_26'):\n",
    "    # Load the raw KITTI data\n",
    "    \n",
    "    #X_train, y_train =preprocess_filtering_data(date=date, serieses = [1,2], dir_path='/home/shahar_zuler/ProjectNexar')\n",
    "    #X_test, y_test = preprocess_data(date=date, state = 'Test', dir_path='/home/shahar_zuler/ProjectNexar')\n",
    "    X_train = np.load('X_train.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "    print (X_train.shape)\n",
    "    X_test = X_train\n",
    "    y_test = y_train\n",
    "    \n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "#     mask = range(num_test)\n",
    "#     X_test = X_test[mask]\n",
    "#     y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define our input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 370, 24, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# define model\n",
    "def stixel_model(X,y,is_training):\n",
    "    conv1 = tf.layers.conv2d(inputs=X, filters=64, kernel_size=[11, 5], \n",
    "                             padding=\"same\", use_bias=True, activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[8, 4], strides=(8, 4))\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=200, kernel_size=[5, 3], \n",
    "                             padding=\"same\", use_bias=True, activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[4, 3], strides=(4,3))\n",
    "    pool2_flat = tf.layers.flatten(inputs=pool2)\n",
    "    dense3 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout3 = tf.layers.dropout(inputs=dense3, rate=0.5, training=is_training)\n",
    "    dense4 = tf.layers.dense(inputs=dropout3, units=2048, activation=tf.nn.relu)\n",
    "    dropout4 = tf.layers.dropout(inputs=dense4, rate=0.5, training=is_training)\n",
    "    y_out = tf.layers.dense(inputs=dropout4, units=47)\n",
    "    return y_out\n",
    "\n",
    "y_out = stixel_model(X,y,is_training)\n",
    "\n",
    "\n",
    "#predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    " #     \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "  #    \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(indices=y, depth=50, axis=-1), logits=y_out)\n",
    "total_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.one_hot(indices=y, depth=47, axis=-1), logits=y_out)\n",
    "#tf.summary.histogram('total_loss',total_loss) ####################\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2, momentum=0.9) \n",
    "# tf.train.MomentumOptimizer(learning_rate=1e-2, momentum=0.9)\n",
    "\n",
    "'''# decay every 10000 steps with a base of 0.5:\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1e-2\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           decay_steps=10000, decay_rate=0.5, staircase=True)\n",
    "\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2, momentum=0.9, decay=0)\n",
    "train_step = optimizer.minimize(mean_loss, global_step=global_step)'''\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate #### 5e-4\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=128, print_every=50,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter( './logs/2/train ', sess.graph) #TB\n",
    "    # have tensorflow compute accuracy\n",
    "    the_prediction = tf.argmax(predict,axis=1)\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,axis=1),y)\n",
    "#     correct_prediction = correct = tf.equal(tf.argmax(tf.nn.sigmoid(predict),axis=1),y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.histogram('accuracy',accuracy) #TB\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "    tf.summary.scalar('mean_loss',mean_loss)\n",
    "\n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "\n",
    "    variables = ['_', mean_loss,correct_prediction, the_prediction ,accuracy] #TB\n",
    "\n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute    \n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now}\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            merge = tf.summary.merge_all() #TB\n",
    "            variables[0] = merge #TB\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "  \n",
    "            summary, loss, corr,pred, _ = session.run(variables,feed_dict=feed_dict) #TB\n",
    "            print('pred: ', pred)\n",
    "            train_writer.add_summary(summary, iter_cnt) #TB\n",
    "    \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Epoch {0}: Iteration {1}: with minibatch training loss = {2:.3g} and accuracy of {3:.2g}\"\\\n",
    "                      .format(e, iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2} Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    \n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, './training_data/stixelnet_model.ckpt')\n",
    "print('Training')\n",
    "run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=features, yd=labels, \n",
    "          epochs=20, batch_size=128, print_every=5, training=train_step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Validation')\n",
    "run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=X_val, yd=y_val, \n",
    "          epochs=1, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from preprocessing.preprocess_func_v02_new import *\n",
    "#preprocess_filtering_data(date='2011_09_26', serieses = [5], dir_path='/home/shahar_zuler/ProjectNexar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Validation2')\n",
    "X5 = np.load('X_train_5.npy')\n",
    "y5 = np.load('y_train_5.npy')\n",
    "run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=X5, yd=y5, \n",
    "          epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
