{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess_func import *\n",
    "%matplotlib inline\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3229, 370, 24, 3)\n",
      "(3229,)\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "ds = preprocess_filtering_data(date='2011_09_26', out_name='train_2', serieses=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorDataset shapes: ((3229, 370, 24, 3), (3229,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[[0.03529412, 0.03921569, 0.03921569],\n",
      "         [0.03529412, 0.03529412, 0.03529412],\n",
      "         [0.03137255, 0.03137255, 0.03529412],\n",
      "         ...,\n",
      "         [0.02745098, 0.03529412, 0.05882353],\n",
      "         [0.03529412, 0.04313726, 0.0627451 ],\n",
      "         [0.03529412, 0.04313726, 0.09411765]],\n",
      "\n",
      "        [[0.02745098, 0.03921569, 0.03921569],\n",
      "         [0.02745098, 0.03529412, 0.03137255],\n",
      "         [0.02745098, 0.03137255, 0.02745098],\n",
      "         ...,\n",
      "         [0.02745098, 0.03137255, 0.05490196],\n",
      "         [0.03137255, 0.04313726, 0.05882353],\n",
      "         [0.03137255, 0.05098039, 0.09019608]],\n",
      "\n",
      "        [[0.02745098, 0.04313726, 0.03529412],\n",
      "         [0.02745098, 0.03529412, 0.03529412],\n",
      "         [0.02745098, 0.03137255, 0.03529412],\n",
      "         ...,\n",
      "         [0.02745098, 0.03529412, 0.03921569],\n",
      "         [0.03137255, 0.03529412, 0.03921569],\n",
      "         [0.03137255, 0.03921569, 0.04705882]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.5411765 , 0.52156866, 0.5058824 ],\n",
      "         [0.5294118 , 0.49411765, 0.41568628],\n",
      "         [0.50980395, 0.47058824, 0.40784314],\n",
      "         ...,\n",
      "         [0.52156866, 0.45882353, 0.4627451 ],\n",
      "         [0.52156866, 0.4862745 , 0.4627451 ],\n",
      "         [0.50980395, 0.49803922, 0.5254902 ]],\n",
      "\n",
      "        [[0.44705883, 0.5137255 , 0.49411765],\n",
      "         [0.4509804 , 0.4627451 , 0.42745098],\n",
      "         [0.49803922, 0.4627451 , 0.43137255],\n",
      "         ...,\n",
      "         [0.4862745 , 0.50980395, 0.5058824 ],\n",
      "         [0.5019608 , 0.49019608, 0.50980395],\n",
      "         [0.5019608 , 0.5058824 , 0.5058824 ]],\n",
      "\n",
      "        [[0.48235294, 0.5019608 , 0.4862745 ],\n",
      "         [0.49411765, 0.4862745 , 0.4392157 ],\n",
      "         [0.5137255 , 0.5058824 , 0.44313726],\n",
      "         ...,\n",
      "         [0.5882353 , 0.5529412 , 0.5137255 ],\n",
      "         [0.54901963, 0.52156866, 0.5137255 ],\n",
      "         [0.5529412 , 0.5019608 , 0.5058824 ]]],\n",
      "\n",
      "\n",
      "       [[[0.03137255, 0.03529412, 0.04313726],\n",
      "         [0.02352941, 0.03529412, 0.03921569],\n",
      "         [0.01568628, 0.03529412, 0.03921569],\n",
      "         ...,\n",
      "         [0.02745098, 0.03921569, 0.05098039],\n",
      "         [0.02352941, 0.03529412, 0.03529412],\n",
      "         [0.03137255, 0.03137255, 0.03137255]],\n",
      "\n",
      "        [[0.03137255, 0.03529412, 0.04705882],\n",
      "         [0.02745098, 0.03529412, 0.04313726],\n",
      "         [0.02352941, 0.03529412, 0.03921569],\n",
      "         ...,\n",
      "         [0.03137255, 0.03529412, 0.05098039],\n",
      "         [0.03137255, 0.03137255, 0.03529412],\n",
      "         [0.03137255, 0.03137255, 0.02352941]],\n",
      "\n",
      "        [[0.02745098, 0.03921569, 0.03529412],\n",
      "         [0.02745098, 0.03529412, 0.03921569],\n",
      "         [0.03529412, 0.03529412, 0.03529412],\n",
      "         ...,\n",
      "         [0.03137255, 0.03529412, 0.05490196],\n",
      "         [0.03137255, 0.03529412, 0.05098039],\n",
      "         [0.02745098, 0.03529412, 0.04313726]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.48235294, 0.4627451 , 0.4392157 ],\n",
      "         [0.49411765, 0.45882353, 0.43529412],\n",
      "         [0.49019608, 0.48235294, 0.43529412],\n",
      "         ...,\n",
      "         [0.49411765, 0.56078434, 0.54509807],\n",
      "         [0.41568628, 0.5137255 , 0.5058824 ],\n",
      "         [0.49019608, 0.4509804 , 0.4627451 ]],\n",
      "\n",
      "        [[0.5019608 , 0.48235294, 0.4627451 ],\n",
      "         [0.49803922, 0.45882353, 0.47843137],\n",
      "         [0.48235294, 0.45490196, 0.4745098 ],\n",
      "         ...,\n",
      "         [0.5019608 , 0.5411765 , 0.58431375],\n",
      "         [0.47058824, 0.5058824 , 0.5058824 ],\n",
      "         [0.5529412 , 0.49019608, 0.44705883]],\n",
      "\n",
      "        [[0.4745098 , 0.4862745 , 0.46666667],\n",
      "         [0.44705883, 0.4745098 , 0.47843137],\n",
      "         [0.41568628, 0.4509804 , 0.46666667],\n",
      "         ...,\n",
      "         [0.5254902 , 0.5176471 , 0.54901963],\n",
      "         [0.50980395, 0.53333336, 0.53333336],\n",
      "         [0.6       , 0.5176471 , 0.54509807]]],\n",
      "\n",
      "\n",
      "       [[[0.02745098, 0.03529412, 0.03529412],\n",
      "         [0.02352941, 0.03529412, 0.03529412],\n",
      "         [0.02352941, 0.03529412, 0.03529412],\n",
      "         ...,\n",
      "         [0.03137255, 0.03529412, 0.03529412],\n",
      "         [0.03137255, 0.03529412, 0.03529412],\n",
      "         [0.03137255, 0.03529412, 0.03529412]],\n",
      "\n",
      "        [[0.02745098, 0.03529412, 0.03529412],\n",
      "         [0.02352941, 0.03137255, 0.03529412],\n",
      "         [0.02352941, 0.03137255, 0.01960784],\n",
      "         ...,\n",
      "         [0.03529412, 0.03137255, 0.03137255],\n",
      "         [0.03137255, 0.02745098, 0.03137255],\n",
      "         [0.03137255, 0.02745098, 0.03137255]],\n",
      "\n",
      "        [[0.02745098, 0.03137255, 0.03529412],\n",
      "         [0.02745098, 0.02745098, 0.03921569],\n",
      "         [0.02745098, 0.03137255, 0.02352941],\n",
      "         ...,\n",
      "         [0.03529412, 0.02745098, 0.03137255],\n",
      "         [0.03529412, 0.02745098, 0.03529412],\n",
      "         [0.03529412, 0.03529412, 0.03137255]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.5803922 , 0.5176471 , 0.49411765],\n",
      "         [0.4627451 , 0.5176471 , 0.49803922],\n",
      "         [0.45490196, 0.47843137, 0.4862745 ],\n",
      "         ...,\n",
      "         [0.58431375, 0.5803922 , 0.53333336],\n",
      "         [0.54901963, 0.5372549 , 0.5058824 ],\n",
      "         [0.41568628, 0.53333336, 0.5137255 ]],\n",
      "\n",
      "        [[0.5411765 , 0.5058824 , 0.5137255 ],\n",
      "         [0.49019608, 0.47058824, 0.50980395],\n",
      "         [0.49803922, 0.47843137, 0.49411765],\n",
      "         ...,\n",
      "         [0.50980395, 0.5764706 , 0.5294118 ],\n",
      "         [0.5411765 , 0.54509807, 0.50980395],\n",
      "         [0.61960787, 0.54509807, 0.52156866]],\n",
      "\n",
      "        [[0.5254902 , 0.45490196, 0.4509804 ],\n",
      "         [0.50980395, 0.4392157 , 0.44313726],\n",
      "         [0.50980395, 0.4745098 , 0.49019608],\n",
      "         ...,\n",
      "         [0.5058824 , 0.5764706 , 0.5372549 ],\n",
      "         [0.54509807, 0.5529412 , 0.5764706 ],\n",
      "         [0.6       , 0.56078434, 0.5686275 ]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         ...,\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ]],\n",
      "\n",
      "        [[1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         ...,\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ]],\n",
      "\n",
      "        [[1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         ...,\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.45490196, 0.41960785, 0.40784314],\n",
      "         [0.35686275, 0.3764706 , 0.3882353 ],\n",
      "         [0.3764706 , 0.38431373, 0.37254903],\n",
      "         ...,\n",
      "         [0.4117647 , 0.40784314, 0.37254903],\n",
      "         [0.4117647 , 0.41960785, 0.40392157],\n",
      "         [0.39607844, 0.4       , 0.43137255]],\n",
      "\n",
      "        [[0.4745098 , 0.43529412, 0.43529412],\n",
      "         [0.34509805, 0.3882353 , 0.42352942],\n",
      "         [0.3647059 , 0.4       , 0.4       ],\n",
      "         ...,\n",
      "         [0.40784314, 0.43529412, 0.41568628],\n",
      "         [0.40784314, 0.43529412, 0.39607844],\n",
      "         [0.40392157, 0.46666667, 0.42352942]],\n",
      "\n",
      "        [[0.47843137, 0.46666667, 0.43529412],\n",
      "         [0.41960785, 0.41960785, 0.42745098],\n",
      "         [0.42745098, 0.46666667, 0.41960785],\n",
      "         ...,\n",
      "         [0.38431373, 0.48235294, 0.44705883],\n",
      "         [0.36078432, 0.42352942, 0.43529412],\n",
      "         [0.4392157 , 0.46666667, 0.4509804 ]]],\n",
      "\n",
      "\n",
      "       [[[1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         ...,\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ]],\n",
      "\n",
      "        [[1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         ...,\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ]],\n",
      "\n",
      "        [[1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         ...,\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.24313726, 0.27058825, 0.27450982],\n",
      "         [0.22352941, 0.2627451 , 0.2784314 ],\n",
      "         [0.1882353 , 0.24313726, 0.27058825],\n",
      "         ...,\n",
      "         [0.13725491, 0.16470589, 0.2       ],\n",
      "         [0.13333334, 0.15686275, 0.18431373],\n",
      "         [0.13333334, 0.15294118, 0.18431373]],\n",
      "\n",
      "        [[0.24705882, 0.23137255, 0.2627451 ],\n",
      "         [0.21568628, 0.23137255, 0.2509804 ],\n",
      "         [0.1764706 , 0.22352941, 0.23529412],\n",
      "         ...,\n",
      "         [0.10980392, 0.15686275, 0.2       ],\n",
      "         [0.09803922, 0.15294118, 0.19215687],\n",
      "         [0.10980392, 0.15686275, 0.19215687]],\n",
      "\n",
      "        [[0.16470589, 0.20784314, 0.25882354],\n",
      "         [0.15686275, 0.20392157, 0.24705882],\n",
      "         [0.15294118, 0.19215687, 0.23921569],\n",
      "         ...,\n",
      "         [0.12941177, 0.16078432, 0.1882353 ],\n",
      "         [0.13333334, 0.16078432, 0.19215687],\n",
      "         [0.13333334, 0.16470589, 0.19215687]]],\n",
      "\n",
      "\n",
      "       [[[1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         ...,\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ]],\n",
      "\n",
      "        [[1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         ...,\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ]],\n",
      "\n",
      "        [[1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         ...,\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ],\n",
      "         [1.        , 1.        , 1.        ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3372549 , 0.34117648, 0.36078432],\n",
      "         [0.3254902 , 0.31764707, 0.3372549 ],\n",
      "         [0.30980393, 0.30980393, 0.3372549 ],\n",
      "         ...,\n",
      "         [0.21960784, 0.24705882, 0.26666668],\n",
      "         [0.2509804 , 0.27058825, 0.27450982],\n",
      "         [0.27058825, 0.2784314 , 0.28627452]],\n",
      "\n",
      "        [[0.40784314, 0.38039216, 0.37254903],\n",
      "         [0.40392157, 0.3882353 , 0.3372549 ],\n",
      "         [0.39215687, 0.39215687, 0.34901962],\n",
      "         ...,\n",
      "         [0.22352941, 0.26666668, 0.2901961 ],\n",
      "         [0.31764707, 0.3019608 , 0.29803923],\n",
      "         [0.3019608 , 0.3137255 , 0.3019608 ]],\n",
      "\n",
      "        [[0.41960785, 0.45490196, 0.4117647 ],\n",
      "         [0.42745098, 0.4627451 , 0.52156866],\n",
      "         [0.4392157 , 0.46666667, 0.47843137],\n",
      "         ...,\n",
      "         [0.3254902 , 0.3137255 , 0.30980393],\n",
      "         [0.34509805, 0.34117648, 0.3137255 ],\n",
      "         [0.34509805, 0.32941177, 0.30588236]]]], dtype=float32), array([29, 29, 29, ..., 46, 46, 46]))\n"
     ]
    }
   ],
   "source": [
    "iter = ds.make_one_shot_iterator()\n",
    "e = iter.get_next()\n",
    "#print(features.shape)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(e)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter = ds.make_one_shot_iterator()\n",
    "features, labels = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "iterator = ds.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "for _ in range(1):\n",
    "  sess.run(iterator.initializer)\n",
    "  while True:\n",
    "    try:\n",
    "      sess.run(next_element)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (3229,) and (3229, 370, 24, 1) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b34dd852e86a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# pass the second value from iter.get_net() as label\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shahar_zuler/assignment1/.envGPU/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(labels, predictions, weights, scope, loss_collection, reduction)\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m     \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquared_difference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     return compute_weighted_loss(\n",
      "\u001b[1;32m/home/shahar_zuler/assignment1/.envGPU/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    754\u001b[0m     \"\"\"\n\u001b[0;32m    755\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (3229,) and (3229, 370, 24, 1) are incompatible"
     ]
    }
   ],
   "source": [
    "# Wrapping all together -> Switch between train and test set\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# # create a placeholder to dynamically switch between batch sizes\n",
    "# # batch_size = tf.placeholder(tf.int64)\n",
    "# # x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "# # dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
    "# # # using two numpy arrays\n",
    "# # train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "# # test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "# # n_batches = len(train_data[0]) // BATCH_SIZE\n",
    "# # iter = dataset.make_initializable_iterator()\n",
    "# # features, labels = iter.get_next()\n",
    "# # make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date='2011_09_26'\n",
    "preprocess_filtering_data(date=date, serieses = [1, 2, 9], dir_path='/home/shahar_zuler/ProjectNexar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "- split to train and validation in a smarter way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(num_training=2000, num_validation=500, num_test=10000, date='2011_09_26'):\n",
    "    # Load the raw KITTI data\n",
    "    \n",
    "    #X_train, y_train =preprocess_filtering_data(date=date, serieses = [1,2], dir_path='/home/shahar_zuler/ProjectNexar')\n",
    "    #X_test, y_test = preprocess_data(date=date, state = 'Test', dir_path='/home/shahar_zuler/ProjectNexar')\n",
    "    X_train = np.load('X_train.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "    print (X_train.shape)\n",
    "    X_test = X_train\n",
    "    y_test = y_train\n",
    "    \n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "#     mask = range(num_test)\n",
    "#     X_test = X_test[mask]\n",
    "#     y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define our input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 370, 24, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# define model\n",
    "def stixel_model(X,y,is_training):\n",
    "    conv1 = tf.layers.conv2d(inputs=X, filters=64, kernel_size=[11, 5], \n",
    "                             padding=\"same\", use_bias=True, activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[8, 4], strides=(8, 4))\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=200, kernel_size=[5, 3], \n",
    "                             padding=\"same\", use_bias=True, activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[4, 3], strides=(4,3))\n",
    "    pool2_flat = tf.layers.flatten(inputs=pool2)\n",
    "    dense3 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout3 = tf.layers.dropout(inputs=dense3, rate=0.5, training=is_training)\n",
    "    dense4 = tf.layers.dense(inputs=dropout3, units=2048, activation=tf.nn.relu)\n",
    "    dropout4 = tf.layers.dropout(inputs=dense4, rate=0.5, training=is_training)\n",
    "    y_out = tf.layers.dense(inputs=dropout4, units=47)\n",
    "    return y_out\n",
    "\n",
    "y_out = stixel_model(X,y,is_training)\n",
    "\n",
    "\n",
    "#predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    " #     \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "  #    \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(indices=y, depth=50, axis=-1), logits=y_out)\n",
    "total_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.one_hot(indices=y, depth=47, axis=-1), logits=y_out)\n",
    "#tf.summary.histogram('total_loss',total_loss) ####################\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2, momentum=0.9) \n",
    "# tf.train.MomentumOptimizer(learning_rate=1e-2, momentum=0.9)\n",
    "\n",
    "'''# decay every 10000 steps with a base of 0.5:\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1e-2\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           decay_steps=10000, decay_rate=0.5, staircase=True)\n",
    "\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2, momentum=0.9, decay=0)\n",
    "train_step = optimizer.minimize(mean_loss, global_step=global_step)'''\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate #### 5e-4\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=128, print_every=50,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter( './logs/2/train ', sess.graph) #TB\n",
    "    # have tensorflow compute accuracy\n",
    "    the_prediction = tf.argmax(predict,axis=1)\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,axis=1),y)\n",
    "#     correct_prediction = correct = tf.equal(tf.argmax(tf.nn.sigmoid(predict),axis=1),y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.histogram('accuracy',accuracy) #TB\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "    tf.summary.scalar('mean_loss',mean_loss)\n",
    "\n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "\n",
    "    variables = ['_', mean_loss,correct_prediction, the_prediction ,accuracy] #TB\n",
    "\n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute    \n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now}\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            merge = tf.summary.merge_all() #TB\n",
    "            variables[0] = merge #TB\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "  \n",
    "            summary, loss, corr,pred, _ = session.run(variables,feed_dict=feed_dict) #TB\n",
    "            print('pred: ', pred)\n",
    "            train_writer.add_summary(summary, iter_cnt) #TB\n",
    "    \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Epoch {0}: Iteration {1}: with minibatch training loss = {2:.3g} and accuracy of {3:.2g}\"\\\n",
    "                      .format(e, iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2} Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    \n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'Dimension' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c4920178b483>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=features, yd=labels, \n\u001b[1;32m---> 11\u001b[1;33m           epochs=20, batch_size=128, print_every=5, training=train_step)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-c3bdbb171d18>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(session, predict, loss_val, Xd, yd, epochs, batch_size, print_every, training, plot_losses)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# shuffle indicies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtrain_indicies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_indicies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'Dimension' and 'int'"
     ]
    }
   ],
   "source": [
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, './training_data/stixelnet_model.ckpt')\n",
    "print('Training')\n",
    "run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=features, yd=labels, \n",
    "          epochs=20, batch_size=128, print_every=5, training=train_step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Validation')\n",
    "run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=X_val, yd=y_val, \n",
    "          epochs=1, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from preprocessing.preprocess_func_v02_new import *\n",
    "#preprocess_filtering_data(date='2011_09_26', serieses = [5], dir_path='/home/shahar_zuler/ProjectNexar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Validation2')\n",
    "X5 = np.load('X_train_5.npy')\n",
    "y5 = np.load('y_train_5.npy')\n",
    "run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=X5, yd=y5, \n",
    "          epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
