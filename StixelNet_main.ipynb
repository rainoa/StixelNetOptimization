{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess_func import *\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3229, 370, 24, 3)\n",
      "(3229,)\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "X,y = preprocess_filtering_data(date='2011_09_26', out_name='train_2', serieses=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H=370\n",
    "W=24\n",
    "C=3\n",
    "img_size_flat = H*W\n",
    "img_shape = (H,W, C)\n",
    "num_classes = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hot = np.zeros((y.shape[0], 47))\n",
    "y_hot[np.arange(y.shape[0]), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3229, 370, 24, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=X.reshape(-1,img_size_flat,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3229, 8880, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[100:]\n",
    "y_train = y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X[:100]\n",
    "y_test = y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(X_train)},\n",
    "    y=np.array(y_train),\n",
    "    num_epochs=None,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.estimator.inputs.numpy_io.numpy_input_fn.<locals>.input_fn>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'x': <tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(128, 8880, 3) dtype=float32>},\n",
       " <tf.Tensor 'random_shuffle_queue_DequeueMany:2' shape=(128,) dtype=int64>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(X_test)},\n",
    "    y=np.array(y_test),\n",
    "    num_epochs=1,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\":np.array(X_test[:20])},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # Args:\n",
    "    #\n",
    "    # features: This is the x-arg from the input_fn.\n",
    "    # labels:   This is the y-arg from the input_fn,\n",
    "    #           see e.g. train_input_fn for these two.\n",
    "    # mode:     Either TRAIN, EVAL, or PREDICT\n",
    "    # params:   User-defined hyper-parameters, e.g. learning-rate.\n",
    "    \n",
    "    # Reference to the tensor named \"x\" in the input-function.\n",
    "    x = features[\"x\"]\n",
    "\n",
    "    # The convolutional layers expect 4-rank tensors\n",
    "    # but x is a 2-rank tensor, so reshape it.\n",
    "    net = tf.reshape(x, [-1, W, H, C])    \n",
    "\n",
    "    # First convolutional layer.\n",
    "    net = tf.layers.conv2d(inputs=net, name='layer_conv1',\n",
    "                           filters=64, kernel_size=(11,5),\n",
    "                           padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=(8,4), strides=1)\n",
    "    #net = tf.nn.lrn(input=net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "    # Second convolutional layer.\n",
    "    net = tf.layers.conv2d(inputs=net, name='layer_conv2',\n",
    "                           filters=200, kernel_size=(5,3),\n",
    "                           padding='same', activation=tf.nn.relu)\n",
    "    #net = tf.nn.lrn(input=net, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=(4,3), strides=1)    \n",
    "\n",
    "    # Flatten to a 2-rank tensor.\n",
    "    #net = tf.contrib.layers.flatten(net)\n",
    "    # Eventually this should be replaced with:\n",
    "    net = tf.layers.flatten(net)\n",
    "\n",
    "    # First fully-connected / dense layer.\n",
    "    # This uses the ReLU activation function.\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc1',\n",
    "                          units=1024, activation=tf.nn.relu)    \n",
    "    \n",
    "    # Second fully-connected / dense layer\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc2',\n",
    "                          units=2048, activation=tf.nn.relu)  \n",
    "    \n",
    "   \n",
    "    # This is the last layer so it does not use an activation function.\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc3',\n",
    "                          units=47)\n",
    "\n",
    "    # Logits output of the neural network.\n",
    "    logits = net\n",
    "\n",
    "    # Softmax output of the neural network.\n",
    "    y_pred = tf.nn.softmax(logits=logits)\n",
    "    \n",
    "    # Classification output of the neural network.\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If the estimator is supposed to be in prediction-mode\n",
    "        # then use the predicted class-number that is output by\n",
    "        # the neural network. Optimization etc. is not needed.\n",
    "        spec = tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=y_pred_cls)\n",
    "    else:\n",
    "        # Otherwise the estimator is supposed to be in either\n",
    "        # training or evaluation-mode. Note that the loss-function\n",
    "        # is also required in Evaluation mode.\n",
    "        \n",
    "        # Define the loss-function to be optimized, by first\n",
    "        # calculating the cross-entropy between the output of\n",
    "        # the neural network and the true labels for the input data.\n",
    "        # This gives the cross-entropy for each image in the batch.\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                       logits=logits)\n",
    "\n",
    "        # Reduce the cross-entropy batch-tensor to a single number\n",
    "        # which can be used in optimization of the neural network.\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        # Define the optimizer for improving the neural network.\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "\n",
    "        # Get the TensorFlow op for doing a single optimization step.\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Define the evaluation metrics,\n",
    "        # in this case the classification accuracy.\n",
    "        metrics = \\\n",
    "        {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels, y_pred_cls)\n",
    "        }\n",
    "\n",
    "        # Wrap all of this in an EstimatorSpec.\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=metrics)\n",
    "        \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"learning_rate\": 1e-4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                               params=params,\n",
    "                               model_dir=\"./checkpoints_tutorial17-2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    }
   ],
   "source": [
    "model.train(input_fn=train_input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = model.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Classification accuracy: {0:.2%}\".format(result[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls_pred = np.array(list(predictions))\n",
    "cls_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iter = ds.make_one_shot_iterator()\n",
    "e = iter.get_next()\n",
    "#print(features.shape)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(e)) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iter = ds.make_one_shot_iterator()\n",
    "features, labels = iter.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_columns = tf.feature_column.numeric_column(features, shape=[370,24,3])\n",
    "type(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_input_fn(filenames, mode=tf.estimator.ModeKeys.EVAL, batch_size=1):\n",
    "  \"\"\"Input function for Estimator API.\"\"\"\n",
    "  def _input_fn():\n",
    "    dataset = ds #tf.data.TFRecordDataset(filenames=filenames)\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    #dataset=dataset.zip(dataset)\n",
    "    dataset = dataset.map(parse_record)\n",
    "\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(2 * batch_size)\n",
    "    #dataset = dataset.map(parse_record)\n",
    "\n",
    "    images, labels = dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "\n",
    "    \n",
    "    features = {'images': images}\n",
    "    return features, labels\n",
    "\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "  \"\"\"Define feature columns.\"\"\"\n",
    "  feature_columns = {\n",
    "    'images': tf.feature_column.numeric_column(\n",
    "        'images', (370, 24, 3)),\n",
    "  }\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "  \"\"\"Define serving function.\"\"\"\n",
    "  receiver_tensor = {\n",
    "      'images': tf.placeholder(shape=[None, 370, 24, 3], dtype=tf.float32)\n",
    "  }\n",
    "  features = {\n",
    "      'images':  receiver_tensor['images']\n",
    "  }\n",
    "  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "  # 1st Convolutional Layer\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=images, filters=64, kernel_size=[5, 5], padding='same',\n",
    "      activation=tf.nn.relu, name='conv1')\n",
    "  pool1 = tf.layers.max_pooling2d(\n",
    "      inputs=conv1, pool_size=[3, 3], strides=2, name='pool1')\n",
    "  norm1 = tf.nn.lrn(\n",
    "      pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "  # 2nd Convolutional Layer\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=norm1, filters=64, kernel_size=[5, 5], padding='same',\n",
    "      activation=tf.nn.relu, name='conv2')\n",
    "  norm2 = tf.nn.lrn(\n",
    "      conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "  pool2 = tf.layers.max_pooling2d(\n",
    "      inputs=norm2, pool_size=[3, 3], strides=2, name='pool2')\n",
    "\n",
    "  # Flatten Layer\n",
    "  shape = pool2.get_shape()\n",
    "  pool2_ = tf.reshape(pool2, [-1, shape[1]*shape[2]*shape[3]])\n",
    "\n",
    "  # 1st Fully Connected Layer\n",
    "  dense1 = tf.layers.dense(\n",
    "      inputs=pool2_, units=384, activation=tf.nn.relu, name='dense1')\n",
    "\n",
    "  # 2nd Fully Connected Layer\n",
    "  dense2 = tf.layers.dense(\n",
    "      inputs=dense1, units=192, activation=tf.nn.relu, name='dense2')\n",
    "\n",
    "  # 3rd Fully Connected Layer (Logits)\n",
    "  logits = tf.layers.dense(\n",
    "      inputs=dense2, units=47, activation=tf.nn.relu, name='logits')\n",
    "\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "  # Create the input layers from the features\n",
    "  feature_columns = list(get_feature_columns().values())\n",
    "\n",
    "  images = tf.feature_column.input_layer(\n",
    "    features=features, feature_columns=feature_columns)\n",
    "\n",
    "  images = tf.reshape(\n",
    "    images, shape=(-1, 370, 24, 3))\n",
    "\n",
    "  # Calculate logits through CNN\n",
    "  logits = inference(images)\n",
    "\n",
    "  if mode in (tf.estimator.ModeKeys.PREDICT, tf.estimator.ModeKeys.EVAL):\n",
    "    predicted_indices = tf.argmax(input=logits, axis=1)\n",
    "    probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
    "\n",
    "  if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    label_indices = tf.argmax(input=labels, axis=1)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=labels, logits=logits)\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    predictions = {\n",
    "        'classes': predicted_indices,\n",
    "        'probabilities': probabilities\n",
    "    }\n",
    "    export_outputs = {\n",
    "        'predictions': tf.estimator.export.PredictOutput(predictions)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, predictions=predictions, export_outputs=export_outputs)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "  #model_dir = os.path.join(FLAGS.output_dir, FLAGS.model_name)\n",
    "\n",
    "  # Create estimator from Keras model.\n",
    "  estimator = tf.estimator.Estimator(\n",
    "      model_fn=model_fn,\n",
    "      model_dir=\"./tmp/model\",\n",
    "      config=tf.estimator.RunConfig(\n",
    "          # save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "          keep_checkpoint_max=5,\n",
    "          tf_random_seed=13))\n",
    "\n",
    "  # Profile Hook.\n",
    "  profile_hook = tf.train.ProfilerHook(\n",
    "      save_steps=True,\n",
    "      output_dir=\"./tmp/model\",\n",
    "      show_dataflow=True,\n",
    "      show_memory=True)\n",
    "\n",
    "  # Specify training data paths, batch size and max steps.\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "      input_fn=generate_input_fn(filenames=\"./tmp/model/train\",\n",
    "                                    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                                    batch_size=28),\n",
    "      max_steps=100,\n",
    "      hooks=[profile_hook]\n",
    "  )\n",
    "\n",
    "  # Currently (2017.12.14) the latest tf only support exporter with keras models.\n",
    "  exporter = tf.estimator.LatestExporter(\n",
    "      name=\"./tmp/model/export\", serving_input_receiver_fn=serving_input_fn,\n",
    "      assets_extra=None, as_text=False, exports_to_keep=5)\n",
    "\n",
    "  # Specify validation data paths, steps for evaluation and exporter specs\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "      input_fn=generate_input_fn(filenames=\"./tmp/model/eval\",\n",
    "                                    mode=tf.estimator.ModeKeys.EVAL,\n",
    "                                    batch_size=28),\n",
    "      steps=1,\n",
    "      name=None,\n",
    "      hooks=None,\n",
    "      exporters=exporter # Iterable of Exporters, or single one or None.\n",
    "  )\n",
    "\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_record(serialized_example):\n",
    "  \"\"\"Parsing CIFAR-10 dataset that is saved in TFRecord format.\"\"\"\n",
    "  features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "      'image': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "\n",
    "  image = tf.decode_raw(features['image'], tf.uint8)\n",
    "  image.set_shape([3 * 370 * 24])\n",
    "  image = tf.reshape(image, [3, 370, 24])\n",
    "  image = tf.cast(tf.transpose(image, [1, 2, 0]), tf.float32)\n",
    "\n",
    "  label = tf.cast(features['label'], tf.int32)\n",
    "  label = tf.one_hot(label, 47)\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "estimator = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[256, 32],\n",
    "    optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "    n_classes=47,\n",
    "    dropout=0.1,\n",
    "    model_dir=\"./tmp/model\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds.output_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "  feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[370,24,3])]\n",
    "  print(feature_columns)\n",
    "\n",
    "  # Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "  classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                          hidden_units=[10, 20, 10],\n",
    "                                          n_classes=47,\n",
    "                                          model_dir=\"/tmp/model2\")\n",
    "  print(classifier)\n",
    "\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X[20:]},\n",
    "    y=y[20:],\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "\n",
    "  \n",
    "  \n",
    "  # Train model.\n",
    "  classifier.train(input_fn=train_input_fn, steps=100)\n",
    "  print('trained')\n",
    "  return classifier\n",
    "\n",
    "def evalf(classifier):\n",
    "  \n",
    "  print('start eval')\n",
    "  \n",
    "  test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X[:2]},\n",
    "    y=y[:2],\n",
    "    num_epochs=None,\n",
    "    shuffle=False)\n",
    "\n",
    "  print('start eval2')\n",
    "  # Evaluate accuracy.\n",
    "  accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "  print(accuracy_score)\n",
    "  print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n",
    "\n",
    " \n",
    "cl=main()\n",
    "print('trained2')\n",
    "#evalf(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X[:2]},\n",
    "    y=y[:2],\n",
    "    num_epochs=None,\n",
    "    shuffle=False)\n",
    "\n",
    "\n",
    "cl.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c=cl.predict(X[3])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": X[3]}\n",
    "\n",
    "cl.predict((input_fn=test_input_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wrapping all together -> Switch between train and test set\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# # create a placeholder to dynamically switch between batch sizes\n",
    "# # batch_size = tf.placeholder(tf.int64)\n",
    "# # x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "# # dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
    "# # # using two numpy arrays\n",
    "# # train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "# # test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "# # n_batches = len(train_data[0]) // BATCH_SIZE\n",
    "# # iter = dataset.make_initializable_iterator()\n",
    "# # features, labels = iter.get_next()\n",
    "# # make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iter = ds.make_one_shot_iterator()\n",
    "features, labels = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date='2011_09_26'\n",
    "preprocess_filtering_data(date=date, serieses = [1, 2, 9], dir_path='/home/shahar_zuler/ProjectNexar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "- split to train and validation in a smarter way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(num_training=2000, num_validation=500, num_test=10000, date='2011_09_26'):\n",
    "    # Load the raw KITTI data\n",
    "    \n",
    "    #X_train, y_train =preprocess_filtering_data(date=date, serieses = [1,2], dir_path='/home/shahar_zuler/ProjectNexar')\n",
    "    #X_test, y_test = preprocess_data(date=date, state = 'Test', dir_path='/home/shahar_zuler/ProjectNexar')\n",
    "    X_train = np.load('X_train.npy')\n",
    "    y_train = np.load('y_train.npy')\n",
    "    print (X_train.shape)\n",
    "    X_test = X_train\n",
    "    y_test = y_train\n",
    "    \n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "#     mask = range(num_test)\n",
    "#     X_test = X_test[mask]\n",
    "#     y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define our input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 370, 24, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# define model\n",
    "def stixel_model(X,y,is_training):\n",
    "    conv1 = tf.layers.conv2d(inputs=X, filters=64, kernel_size=[11, 5], \n",
    "                             padding=\"same\", use_bias=True, activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[8, 4], strides=(8, 4))\n",
    "    conv2 = tf.layers.conv2d(inputs=pool1, filters=200, kernel_size=[5, 3], \n",
    "                             padding=\"same\", use_bias=True, activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[4, 3], strides=(4,3))\n",
    "    pool2_flat = tf.layers.flatten(inputs=pool2)\n",
    "    dense3 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout3 = tf.layers.dropout(inputs=dense3, rate=0.5, training=is_training)\n",
    "    dense4 = tf.layers.dense(inputs=dropout3, units=2048, activation=tf.nn.relu)\n",
    "    dropout4 = tf.layers.dropout(inputs=dense4, rate=0.5, training=is_training)\n",
    "    y_out = tf.layers.dense(inputs=dropout4, units=47)\n",
    "    return y_out\n",
    "\n",
    "y_out = stixel_model(X,y,is_training)\n",
    "\n",
    "\n",
    "#predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    " #     \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "  #    \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(indices=y, depth=50, axis=-1), logits=y_out)\n",
    "total_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.one_hot(indices=y, depth=47, axis=-1), logits=y_out)\n",
    "#tf.summary.histogram('total_loss',total_loss) ####################\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2, momentum=0.9) \n",
    "# tf.train.MomentumOptimizer(learning_rate=1e-2, momentum=0.9)\n",
    "\n",
    "'''# decay every 10000 steps with a base of 0.5:\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1e-2\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           decay_steps=10000, decay_rate=0.5, staircase=True)\n",
    "\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-2, momentum=0.9, decay=0)\n",
    "train_step = optimizer.minimize(mean_loss, global_step=global_step)'''\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate #### 5e-4\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=128, print_every=50,\n",
    "              training=None, plot_losses=False):\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter( './logs/2/train ', sess.graph) #TB\n",
    "    # have tensorflow compute accuracy\n",
    "    the_prediction = tf.argmax(predict,axis=1)\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,axis=1),y)\n",
    "#     correct_prediction = correct = tf.equal(tf.argmax(tf.nn.sigmoid(predict),axis=1),y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.histogram('accuracy',accuracy) #TB\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "    tf.summary.scalar('mean_loss',mean_loss)\n",
    "\n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "\n",
    "    variables = ['_', mean_loss,correct_prediction, the_prediction ,accuracy] #TB\n",
    "\n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute    \n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now}\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            merge = tf.summary.merge_all() #TB\n",
    "            variables[0] = merge #TB\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "  \n",
    "            summary, loss, corr,pred, _ = session.run(variables,feed_dict=feed_dict) #TB\n",
    "            print('pred: ', pred)\n",
    "            train_writer.add_summary(summary, iter_cnt) #TB\n",
    "    \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Epoch {0}: Iteration {1}: with minibatch training loss = {2:.3g} and accuracy of {3:.2g}\"\\\n",
    "                      .format(e, iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2} Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    \n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.save(sess, './training_data/stixelnet_model.ckpt')\n",
    "print('Training')\n",
    "run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=features, yd=labels, \n",
    "          epochs=20, batch_size=128, print_every=5, training=train_step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Validation')\n",
    "run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=X_val, yd=y_val, \n",
    "          epochs=1, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from preprocessing.preprocess_func_v02_new import *\n",
    "#preprocess_filtering_data(date='2011_09_26', serieses = [5], dir_path='/home/shahar_zuler/ProjectNexar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Validation2')\n",
    "X5 = np.load('X_train_5.npy')\n",
    "y5 = np.load('y_train_5.npy')\n",
    "run_model(session=sess, predict=y_out, loss_val=mean_loss, Xd=X5, yd=y5, \n",
    "          epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
