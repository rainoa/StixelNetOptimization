{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########now part 2: decode and train#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12455067702093542274\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11271654605\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2800243769426591478\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "#from preprocess_func_new import *\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "from os.path import expanduser\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets/stixels'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = os.path.join('..','datasets','stixels')\n",
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrec_batch_size=1 #for path name only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrec_train_directory = os.path.join('..','datasets','stixels','train','tfrec_batch_size_'+str(tfrec_batch_size)+'_percent_'+str(percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of paths to train tfrecs:\n",
    "path_tfrecords_train_lst=[]\n",
    "path_tfrecords_train = os.path.join(img_path, 'train')\n",
    "for root, dirs, files in os.walk(tfrec_train_directory):\n",
    "    for file in files:\n",
    "        if '.tfrecord' in file:\n",
    "            path_tfrecords_train_lst.append(os.path.join(tfrec_train_directory,file))\n",
    "        else:\n",
    "            print('WARNING: file ' + file + 'looks suspicious. does it belong here?')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortening the list for experiments\n",
    "path_tfrecords_train_lst = path_tfrecords_train_lst[:100]\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "H=370 \n",
    "W=24\n",
    "C=3\n",
    "img_shape = (H, W, C)\n",
    "num_classes = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(serialized):\n",
    "    # Define a dict with the data-names and types we expect to\n",
    "    # find in the TFRecords file.\n",
    "    # It is a bit awkward that this needs to be specified again,\n",
    "    # because it could have been written in the header of the\n",
    "    # TFRecords file instead.\n",
    "    features = \\\n",
    "        {\n",
    "            'image': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "\n",
    "    # Parse the serialized data so we get a dict with our data.\n",
    "    parsed_example = tf.parse_single_example(serialized=serialized,\n",
    "                                             features=features)\n",
    "\n",
    "    # Get the image as raw bytes.\n",
    "    image_raw = parsed_example['image']\n",
    "\n",
    "    # Decode the raw bytes so it becomes a tensor with type.\n",
    "    #######image = tf.decode_raw(image_raw, tf.int32) ####\n",
    "    image = tf.image.decode_png(image_raw, channels=3, dtype=tf.uint8) \n",
    "    #image = tf.cast(image, tf.int32)\n",
    "\n",
    "    # The type is now uint8 but we need it to be float.\n",
    "    \n",
    "    image = tf.cast(image, tf.float32) ####\n",
    "    \n",
    "    # Get the label associated with the image.\n",
    "    label = parsed_example['label']\n",
    "\n",
    "    # The image and label are now correct TensorFlow types.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## a function to print tensor values\n",
    "def print_tensor(tensor):\n",
    "    dataset = tf.data.TFRecordDataset(filenames=[tensor])\n",
    "    dataset = dataset.map(parse)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images_batch, labels_batch = iterator.get_next()\n",
    "    x = {'image': images_batch}\n",
    "    y = labels_batch\n",
    "    init_op = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        print (sess.run(y))\n",
    "        #print (sess.run(x))\n",
    "        #return(sess.run(x))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ try to print the tensors\n",
    "for t in path_tfrecords_train_lst[:100]:\n",
    "    #print (t)\n",
    "    print_tensor(t)\n",
    "    #print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 24, 3)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# print shape of tensor\n",
    "x2=tf.Session().run(x['image'])\n",
    "x2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa46a8d0e48>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa46a75c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################. show the parsed stixel\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.pyplot.imshow(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames, train, batch_size=batch_size, buffer_size=100000): \n",
    "    # Args:\n",
    "    # filenames:   Filenames for the TFRecords files.\n",
    "    # train:       Boolean whether training (True) or testing (False).\n",
    "    # batch_size:  Return batches of this size.\n",
    "    # buffer_size: Read buffers of this size. The random shuffling\n",
    "    #              is done on the buffer, so it must be big enough.\n",
    "\n",
    "    # Create a TensorFlow Dataset-object which has functionality\n",
    "    # for reading and shuffling data from TFRecords files.\n",
    "    dataset = tf.data.TFRecordDataset(filenames=filenames)\n",
    "\n",
    "    # Parse the serialized data in the TFRecords files.\n",
    "    # This returns TensorFlow tensors for the image and labels.\n",
    "    dataset = dataset.map(parse)\n",
    "    \n",
    "    if train:\n",
    "        # If training then read a buffer of the given size and\n",
    "        # randomly shuffle it.\n",
    "        ######dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "        # Allow infinite reading of the data.\n",
    "        num_repeat = None #-1\n",
    "    else:\n",
    "        # If testing then don't shuffle the data.\n",
    "        \n",
    "        # Only go through the data once.\n",
    "        num_repeat = 1\n",
    "\n",
    "    # Repeat the dataset the given number of times.\n",
    "    dataset = dataset.repeat(num_repeat)\n",
    "    \n",
    "    # Get a batch of data with the given size.\n",
    "    #dataset = dataset.batch(batch_size)\n",
    "    #dataset = tf.contrib.data.batch_and_drop_remainder(batch_size)\n",
    "    dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "    print(dataset.output_shapes)  # ==> \"(16,)\" (the batch dimension is known)\n",
    "    \n",
    "    # Create an iterator for the dataset and the above modifications.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    # Get the next batch of images and labels.\n",
    "    images_batch, labels_batch = iterator.get_next()\n",
    "\n",
    "    # The input-function must return a dict wrapping the images.\n",
    "    x = {'image': images_batch}\n",
    "    y = labels_batch\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return input_fn(filenames=path_tfrecords_train_lst, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # Args:\n",
    "    #\n",
    "    # features: This is the x-arg from the input_fn.\n",
    "    # labels:   This is the y-arg from the input_fn.\n",
    "    # mode:     Either TRAIN, EVAL, or PREDICT\n",
    "    # params:   User-defined hyper-parameters, e.g. learning-rate.\n",
    "    \n",
    "    # Reference to the tensor named \"image\" in the input-function.\n",
    "    x = features[\"image\"]\n",
    "    # The convolutional layers expect 4-rank tensors\n",
    "    # but x is a 2-rank tensor, so reshape it.\n",
    "    net = tf.reshape(x, [-1,W,H,C])\n",
    "\n",
    "    net = tf.layers.conv2d(inputs=net, name='layer_conv1',\n",
    "                           filters=32, kernel_size=(3,3),\n",
    "                           activation=None, strides=(2,2),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    net = tf.contrib.layers.separable_conv2d(inputs=net, num_outputs=None,kernel_size=(3,3), stride=(1, 1),\n",
    "                               activation_fn=None, depth_multiplier=1,\n",
    "                               weights_initializer=tf.contrib.layers.xavier_initializer(seed=481),\n",
    "                               padding='SAME'\n",
    "                              )\n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    net = tf.layers.conv2d(inputs=net, name='point_wize3',\n",
    "                           filters=32, kernel_size=(1,1),\n",
    "                           activation=None, strides=(1,1),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    net = tf.contrib.layers.separable_conv2d(inputs=net, num_outputs=None, kernel_size=(3,3), stride=(2,2),\n",
    "                                activation_fn=None, depth_multiplier=1,\n",
    "                                weights_initializer=tf.contrib.layers.xavier_initializer(seed=481),\n",
    "                                padding='SAME'\n",
    "                               )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    net = tf.layers.conv2d(inputs=net, name='point_wize5',\n",
    "                           filters=64, kernel_size=(1,1),\n",
    "                           activation=None, strides=(1,1),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    ####\n",
    "\n",
    "    net = tf.contrib.layers.separable_conv2d(inputs=net, num_outputs=None, kernel_size=(3,3), stride=(1,1),\n",
    "                                activation_fn=None, depth_multiplier=1,\n",
    "                                weights_initializer=tf.contrib.layers.xavier_initializer(seed=481),\n",
    "                                padding='SAME'\n",
    "                                    )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)  \n",
    "    \n",
    "    net = tf.layers.conv2d(inputs=net, name='point_wize7',\n",
    "                           filters=128, kernel_size=(1,1),\n",
    "                           activation=None, strides=(1,1),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    \n",
    "    net = tf.contrib.layers.separable_conv2d(inputs=net, num_outputs=None, kernel_size=(3,3), stride=(2,2),\n",
    "                                activation_fn=None, depth_multiplier=1,\n",
    "                                weights_initializer=tf.contrib.layers.xavier_initializer(seed=481),\n",
    "                                padding='SAME'\n",
    "                               )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)    \n",
    "    \n",
    "    net = tf.layers.conv2d(inputs=net, name='point_wize9',\n",
    "                           filters=128, kernel_size=(1,1),\n",
    "                           activation=None, strides=(1,1),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    ####\n",
    "\n",
    "    net = tf.contrib.layers.separable_conv2d(inputs=net,num_outputs=None, kernel_size=(3,3), stride=(1,1),\n",
    "                                activation_fn=None, depth_multiplier=1,\n",
    "                                weights_initializer=tf.contrib.layers.xavier_initializer(seed=481),\n",
    "                                padding='SAME'\n",
    "                               )\n",
    "\n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)  \n",
    "    \n",
    "    net = tf.layers.conv2d(inputs=net, name='point_wize11',\n",
    "                           filters=256, kernel_size=(1,1),\n",
    "                           activation=None, strides=(1,1),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    net = tf.contrib.layers.separable_conv2d(inputs=net, num_outputs=None, kernel_size=(3,3), stride=(2,2),\n",
    "                                activation_fn=None, depth_multiplier=1,\n",
    "                                weights_initializer=tf.contrib.layers.xavier_initializer(seed=481),\n",
    "                                padding='SAME'\n",
    "                               )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)    \n",
    "    \n",
    "    net = tf.layers.conv2d(inputs=net, name='point_wize13',\n",
    "                           filters=256, kernel_size=(1,1),\n",
    "                           activation=None, strides=(1,1),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    for i in range(5):\n",
    "        net = tf.contrib.layers.separable_conv2d(inputs=net, num_outputs=None, kernel_size=(3,3), stride=(1,1),\n",
    "                                    activation_fn=None, depth_multiplier=1,\n",
    "                                    weights_initializer=tf.contrib.layers.xavier_initializer(seed=481),\n",
    "                                    padding='SAME'\n",
    "                                   )\n",
    "\n",
    "        net = tf.layers.batch_normalization(inputs=net)\n",
    "\n",
    "        net = tf.nn.relu6(features=net)  \n",
    "        \n",
    "        net = tf.layers.conv2d(inputs=net, name='point_wize'+str(i+14),\n",
    "                           filters=512, kernel_size=(1,1),\n",
    "                           activation=None, strides=(1,1),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "        net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "        net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    \n",
    "    net = tf.contrib.layers.separable_conv2d(inputs=net, num_outputs=None, kernel_size=(3,3), stride=(2,2),\n",
    "                                activation_fn=None, depth_multiplier=1,\n",
    "                                weights_initializer=tf.contrib.layers.xavier_initializer(seed=481),\n",
    "                                padding='SAME'\n",
    "                               )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net) \n",
    "    \n",
    "    net = tf.layers.conv2d(inputs=net, name='point_wize19',\n",
    "                           filters=512, kernel_size=(1,1),\n",
    "                           activation=None, strides=(1,1),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    net = tf.contrib.layers.separable_conv2d(inputs=net, num_outputs=None, kernel_size=(3,3), stride=(2,2),\n",
    "                                activation_fn=None, depth_multiplier=1,\n",
    "                                weights_initializer=tf.contrib.layers.xavier_initializer(seed=481),\n",
    "                                padding='SAME'\n",
    "                               )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)    \n",
    "    \n",
    "    net = tf.layers.conv2d(inputs=net, name='point_wize20',\n",
    "                           filters=1024, kernel_size=(1,1),\n",
    "                           activation=None, strides=(1,1),\n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(seed=481)\n",
    "                           )\n",
    "    \n",
    "    net = tf.layers.batch_normalization(inputs=net)\n",
    "    \n",
    "    net = tf.nn.relu6(features=net)\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    net = tf.layers.average_pooling2d(inputs=net, pool_size=(1,6), strides=(1,1), name='layer_pool21')\n",
    "    \n",
    "    # Flatten to a 2-rank tensor.\n",
    "    #net = tf.contrib.layers.flatten(net)\n",
    "    # Eventually this should be replaced with:\n",
    "    net = tf.layers.flatten(net)\n",
    "\n",
    "\n",
    "    # This is the last layer so it does not use an activation function.\n",
    "    net = tf.layers.dense(inputs=net, name='layer_fc22',\n",
    "                          units=47,\n",
    "                          kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                          ) \n",
    "\n",
    "    # Logits output of the neural network.\n",
    "    logits = net\n",
    "\n",
    "    # Softmax output of the neural network.\n",
    "    y_pred = tf.nn.softmax(logits=logits)\n",
    "    \n",
    "    # Classification output of the neural network.\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1) \n",
    "   \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If the estimator is supposed to be in prediction-mode\n",
    "        # then use the predicted class-number that is output by\n",
    "        # the neural network. Optimization etc. is not needed.\n",
    "        \n",
    "        spec = tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=y_pred)# _cls)\n",
    "    else:\n",
    "        # Otherwise the estimator is supposed to be in either\n",
    "        # training or evaluation-mode. Note that the loss-function\n",
    "        # is also required in Evaluation mode.\n",
    "        \n",
    "        # Define the loss-function to be optimized, by first\n",
    "        # calculating the cross-entropy between the output of\n",
    "        # the neural network and the true labels for the input data.\n",
    "        # This gives the cross-entropy for each image in the batch.\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                       logits=logits)\n",
    "\n",
    "        # Reduce the cross-entropy batch-tensor to a single number\n",
    "        # which can be used in optimization of the neural network.\n",
    "        tf.argmax(logits)\n",
    "        #loss = tf.losses.mean_squared_error(labels=labels, predictions=logits)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    " #######################\n",
    "        lr = 1e-10\n",
    "        step_rate = 1000\n",
    "        decay = 0.5 #if this equals 1 the lr stays the same\n",
    "\n",
    "        #global_step = tf.Variable(0, trainable=False)\n",
    "        #increment_global_step = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "        learning_rate = tf.train.exponential_decay(lr, global_step=tf.train.get_or_create_global_step(), \n",
    "                                           decay_steps=step_rate, decay_rate=decay, staircase=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Define the optimizer for improving the neural network.\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) \n",
    "        # Get the TensorFlow op for doing a single optimization step.\n",
    "        train_op = optimizer.minimize(loss=loss, global_step = tf.train.get_or_create_global_step())\n",
    "        \n",
    "\n",
    "#############################        \n",
    "\n",
    "\n",
    "        # Define the evaluation metrics,\n",
    "        # in this case the classification accuracy.\n",
    "        metrics = \\\n",
    "        {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels, y_pred_cls) #TODO change acc method\n",
    "        }\n",
    "\n",
    "        # Wrap all of this in an EstimatorSpec.\n",
    "        spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=metrics)\n",
    "        \n",
    "        \n",
    "#         with tf.Session() as sess:\n",
    "#             print('Learning rate: %f' % (sess.run(optimizer._learning_rate)))\n",
    "        \n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"learning_rate\": 1e-10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_and_comment(model_dir):\n",
    "    home = expanduser(\"~\")\n",
    "    log_name=os.path.join('logs/', model_dir + '.txt')\n",
    "    \n",
    "    if os.path.isdir(model_dir):\n",
    "        print('INFO: dir with name ' + model_dir + ' already exist.')\n",
    "    \n",
    "    new_comment=input('Please add a comment\\n')\n",
    "    \n",
    "    if os.path.exists(log_name):\n",
    "        append_write = 'a' # append if already exists\n",
    "    else:\n",
    "        append_write = 'w' # make a new file if not\n",
    "    \n",
    "    model_log = open(log_name,append_write)\n",
    "    model_log.write(home +' : '+ new_comment + '\\n')\n",
    "    model_log.close()\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: dir with name ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW already exist.\n",
      "Please add a comment\n",
      "1e-10\n",
      "INFO:tensorflow:Using config: {'_model_dir': './ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff5862f5550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_dir = './ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW'#_adam' #'./ckpts_<day>_<month>_<architecture>_<main_change>'\n",
    "make_dir_and_comment(model_dir) \n",
    "model = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                               params=params,\n",
    "                               model_dir=model_dir,\n",
    "                               config=tf.estimator.RunConfig(save_checkpoints_steps=1000, save_summary_steps=100)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorShape([Dimension(32), Dimension(None), Dimension(None), Dimension(3)]), TensorShape([Dimension(32)]))\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt-280002\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 280003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.4207299, step = 280002\n",
      "INFO:tensorflow:global_step/sec: 8.25876\n",
      "INFO:tensorflow:loss = 3.304316, step = 280102 (12.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1228\n",
      "INFO:tensorflow:loss = 3.4585538, step = 280202 (9.878 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0935\n",
      "INFO:tensorflow:loss = 3.3104243, step = 280302 (9.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0996\n",
      "INFO:tensorflow:loss = 3.4396026, step = 280402 (9.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1412\n",
      "INFO:tensorflow:loss = 3.4413824, step = 280502 (9.861 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1105\n",
      "INFO:tensorflow:loss = 3.487143, step = 280602 (9.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0177\n",
      "INFO:tensorflow:loss = 3.447699, step = 280702 (9.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0441\n",
      "INFO:tensorflow:loss = 3.3468804, step = 280802 (9.954 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0625\n",
      "INFO:tensorflow:loss = 3.4058323, step = 280902 (9.939 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 281003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.01058\n",
      "INFO:tensorflow:loss = 3.4493937, step = 281002 (11.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2111\n",
      "INFO:tensorflow:loss = 3.5141263, step = 281102 (9.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2527\n",
      "INFO:tensorflow:loss = 3.3465114, step = 281202 (9.754 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1556\n",
      "INFO:tensorflow:loss = 3.517858, step = 281302 (9.848 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1323\n",
      "INFO:tensorflow:loss = 3.4141169, step = 281402 (9.870 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1659\n",
      "INFO:tensorflow:loss = 3.431458, step = 281502 (9.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2669\n",
      "INFO:tensorflow:loss = 3.3686748, step = 281602 (9.740 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1654\n",
      "INFO:tensorflow:loss = 3.3900647, step = 281702 (9.838 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1975\n",
      "INFO:tensorflow:loss = 3.3768916, step = 281802 (9.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1008\n",
      "INFO:tensorflow:loss = 3.3780327, step = 281902 (9.900 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.02691\n",
      "INFO:tensorflow:loss = 3.5363631, step = 282002 (11.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0767\n",
      "INFO:tensorflow:loss = 3.5609028, step = 282102 (9.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0419\n",
      "INFO:tensorflow:loss = 3.3606179, step = 282202 (9.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0225\n",
      "INFO:tensorflow:loss = 3.388356, step = 282302 (9.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0365\n",
      "INFO:tensorflow:loss = 3.459205, step = 282402 (9.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.051\n",
      "INFO:tensorflow:loss = 3.3043516, step = 282502 (9.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0168\n",
      "INFO:tensorflow:loss = 3.476512, step = 282602 (9.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0866\n",
      "INFO:tensorflow:loss = 3.4532866, step = 282702 (9.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0634\n",
      "INFO:tensorflow:loss = 3.4416752, step = 282802 (9.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0773\n",
      "INFO:tensorflow:loss = 3.5381324, step = 282902 (9.923 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 283003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.01454\n",
      "INFO:tensorflow:loss = 3.3841124, step = 283002 (11.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0521\n",
      "INFO:tensorflow:loss = 3.4723394, step = 283102 (9.950 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0075\n",
      "INFO:tensorflow:loss = 3.3783338, step = 283202 (9.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.97813\n",
      "INFO:tensorflow:loss = 3.4940543, step = 283302 (10.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.94124\n",
      "INFO:tensorflow:loss = 3.438842, step = 283402 (10.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.9672\n",
      "INFO:tensorflow:loss = 3.416881, step = 283502 (10.034 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0587\n",
      "INFO:tensorflow:loss = 3.4180427, step = 283602 (9.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0203\n",
      "INFO:tensorflow:loss = 3.4197807, step = 283702 (9.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0152\n",
      "INFO:tensorflow:loss = 3.380465, step = 283802 (9.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0451\n",
      "INFO:tensorflow:loss = 3.4042408, step = 283902 (9.956 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 284003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 8.96224\n",
      "INFO:tensorflow:loss = 3.4323835, step = 284002 (11.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0702\n",
      "INFO:tensorflow:loss = 3.4297438, step = 284102 (9.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0669\n",
      "INFO:tensorflow:loss = 3.360435, step = 284202 (9.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0091\n",
      "INFO:tensorflow:loss = 3.2959237, step = 284302 (9.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0191\n",
      "INFO:tensorflow:loss = 3.4273992, step = 284402 (9.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.99194\n",
      "INFO:tensorflow:loss = 3.451756, step = 284502 (10.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.96011\n",
      "INFO:tensorflow:loss = 3.436107, step = 284602 (10.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.98878\n",
      "INFO:tensorflow:loss = 3.6158004, step = 284702 (10.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.17\n",
      "INFO:tensorflow:loss = 3.5399618, step = 284802 (9.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.96018\n",
      "INFO:tensorflow:loss = 3.3625586, step = 284902 (10.040 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 285003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.06938\n",
      "INFO:tensorflow:loss = 3.3305848, step = 285002 (11.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.99648\n",
      "INFO:tensorflow:loss = 3.4876022, step = 285102 (10.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0753\n",
      "INFO:tensorflow:loss = 3.406239, step = 285202 (9.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.9991\n",
      "INFO:tensorflow:loss = 3.3140318, step = 285302 (10.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0503\n",
      "INFO:tensorflow:loss = 3.4094064, step = 285402 (9.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.098\n",
      "INFO:tensorflow:loss = 3.354452, step = 285502 (9.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0706\n",
      "INFO:tensorflow:loss = 3.3617392, step = 285602 (9.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0802\n",
      "INFO:tensorflow:loss = 3.3799827, step = 285702 (9.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0821\n",
      "INFO:tensorflow:loss = 3.472384, step = 285802 (9.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0388\n",
      "INFO:tensorflow:loss = 3.5169697, step = 285902 (9.960 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 286003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 8.96313\n",
      "INFO:tensorflow:loss = 3.4947405, step = 286002 (11.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.97474\n",
      "INFO:tensorflow:loss = 3.3831413, step = 286102 (10.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0012\n",
      "INFO:tensorflow:loss = 3.4036498, step = 286202 (10.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.99171\n",
      "INFO:tensorflow:loss = 3.3611116, step = 286302 (10.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0441\n",
      "INFO:tensorflow:loss = 3.3879223, step = 286402 (9.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.97084\n",
      "INFO:tensorflow:loss = 3.4197044, step = 286502 (10.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0583\n",
      "INFO:tensorflow:loss = 3.3596013, step = 286602 (9.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.94391\n",
      "INFO:tensorflow:loss = 3.2348464, step = 286702 (10.058 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.95297\n",
      "INFO:tensorflow:loss = 3.4640918, step = 286802 (10.046 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 9.97308\n",
      "INFO:tensorflow:loss = 3.318682, step = 286902 (10.026 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 287003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 8.9782\n",
      "INFO:tensorflow:loss = 3.3367162, step = 287002 (11.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0611\n",
      "INFO:tensorflow:loss = 3.3053002, step = 287102 (9.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0673\n",
      "INFO:tensorflow:loss = 3.46056, step = 287202 (9.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2085\n",
      "INFO:tensorflow:loss = 3.452015, step = 287302 (9.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2513\n",
      "INFO:tensorflow:loss = 3.4109569, step = 287402 (9.755 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1565\n",
      "INFO:tensorflow:loss = 3.3428495, step = 287502 (9.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1251\n",
      "INFO:tensorflow:loss = 3.388183, step = 287602 (9.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.026\n",
      "INFO:tensorflow:loss = 3.3466778, step = 287702 (9.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.9656\n",
      "INFO:tensorflow:loss = 3.3108659, step = 287802 (10.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0354\n",
      "INFO:tensorflow:loss = 3.3671098, step = 287902 (9.965 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 288003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.01786\n",
      "INFO:tensorflow:loss = 3.598878, step = 288002 (11.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0583\n",
      "INFO:tensorflow:loss = 3.3597236, step = 288102 (9.942 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1308\n",
      "INFO:tensorflow:loss = 3.3876925, step = 288202 (9.872 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0969\n",
      "INFO:tensorflow:loss = 3.3856478, step = 288302 (9.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.95358\n",
      "INFO:tensorflow:loss = 3.4423838, step = 288402 (10.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1321\n",
      "INFO:tensorflow:loss = 3.4843273, step = 288502 (9.868 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1671\n",
      "INFO:tensorflow:loss = 3.5169458, step = 288602 (9.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1411\n",
      "INFO:tensorflow:loss = 3.345934, step = 288702 (9.861 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0445\n",
      "INFO:tensorflow:loss = 3.484608, step = 288802 (9.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0742\n",
      "INFO:tensorflow:loss = 3.4110062, step = 288902 (9.926 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 289003 into ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.0711\n",
      "INFO:tensorflow:loss = 3.6072614, step = 289002 (11.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1282\n",
      "INFO:tensorflow:loss = 3.4429185, step = 289102 (9.875 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0855\n",
      "INFO:tensorflow:loss = 3.4035735, step = 289202 (9.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0756\n",
      "INFO:tensorflow:loss = 3.34177, step = 289302 (9.926 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1504\n",
      "INFO:tensorflow:loss = 3.5734358, step = 289402 (9.851 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-f2a35d1e8a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    544\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1023\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(input_fn=train_input_fn, steps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DONE TRAIN###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrec_val_batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrec_val_directory = os.path.join('..','datasets','stixels','val','tfrec_batch_size_'+str(tfrec_val_batch_size)+'_percent_'+str(percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tfrecords_val_lst=[]\n",
    "path_tfrecords_val = os.path.join(img_path, 'val')\n",
    "for root, dirs, files in os.walk(tfrec_val_directory):\n",
    "    for file in files:\n",
    "        if '.tfrecord' in file:\n",
    "            path_tfrecords_val_lst.append(os.path.join(tfrec_val_directory,file))\n",
    "        else:\n",
    "            print('WARNING: file ' + file + 'looks suspicious. does it belong here?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_input_fn():\n",
    "    return input_fn(path_tfrecords_val_lst[7000:8500], train=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorShape([Dimension(32), Dimension(None), Dimension(None), Dimension(3)]), TensorShape([Dimension(32)]))\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-26-22:00:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt-180002\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-26-22:00:04\n",
      "INFO:tensorflow:Saving dict for global step 180002: accuracy = 0.05027174, global_step = 180002, loss = 3.4448183\n"
     ]
    }
   ],
   "source": [
    "val_result = model.evaluate(input_fn=val_input_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.05027174, 'global_step': 180002, 'loss': 3.4448183}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification val accuracy: {0:.2%}\".format(val_result[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrec_test_batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrec_test_directory = os.path.join('..','datasets','stixels','test','tfrec_batch_size_'+str(tfrec_test_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tfrecords_test_lst=[]\n",
    "path_tfrecords_test = os.path.join(img_path, 'test')\n",
    "for root, dirs, files in os.walk(tfrec_test_directory):\n",
    "    for file in files:\n",
    "        if '.tfrecord' in file:\n",
    "            path_tfrecords_test_lst.append(os.path.join(tfrec_test_directory,file))\n",
    "        else:\n",
    "            print('WARNING: file ' + file + 'looks suspicious. does it belong here?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_input_fn():\n",
    "    return input_fn(filenames=path_tfrecords_train_lst[:5000], train=False) #path_tfrecords_test_lst[1000:3000], train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorShape([Dimension(32), Dimension(None), Dimension(None), Dimension(3)]), TensorShape([Dimension(32)]))\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-25-18:18:04\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./ckpts_25_5_mobilenet1_3layers_all_examples_v4/model.ckpt-34006\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-25-18:18:09\n",
      "INFO:tensorflow:Saving dict for global step 34006: accuracy = 0.075921476, global_step = 34006, loss = 3.2370234\n"
     ]
    }
   ],
   "source": [
    "test_result = model.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification test accuracy: 7.59%\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification test accuracy: {0:.2%}\".format(test_result[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_csv_test_path = os.path.join(img_path,'train', 'sum_csv') #TEST\n",
    "labels_test=pd.read_csv(os.path.join(sum_csv_test_path,'labels_train_'+str(percent)+'percent.csv'))\n",
    "test_names_list=list(labels_test['Name'])\n",
    "image_paths_test=[]\n",
    "for name in test_names_list:\n",
    "    image_paths_test.append(os.path.join(img_path, 'train', name+'.png')) #maybe no need to add '.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_paths):\n",
    "    # Load the images from disk.\n",
    "    images = [imread(path) for path in image_paths]\n",
    "    # Convert to a numpy array and return it.\n",
    "    return np.asarray(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO:SHUFFLE!\n",
    "some_num=100\n",
    "some_images = load_images(image_paths=image_paths_test[0:0+some_num])\n",
    "some_images_cls = np.array(labels_test['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"image\": some_images.astype(np.float32)},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(input_fn=predict_input_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./ckpts_26_5_mobilenet1_all_16_layers_all_examples_PW/model.ckpt-180002\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       ...,\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848]], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred = np.array(list(predictions))\n",
    "cls_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 47)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       ...,\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848],\n",
       "       [0.00010882, 0.00010766, 0.00010967, ..., 0.00010558, 0.00010859,\n",
       "        0.01937848]], dtype=float32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1300,    0,  200,    0,    0,    0,  100,    0,  100,    0,    0,\n",
       "           0,  100,    0,  100,    0,    0,    0,  200,    0,    0,    0,\n",
       "         200,  100,    0,  300,  100,  200,    0,    0,  100,  200,    0,\n",
       "         200,  100,  300,  100,    0,  100,  100,    0,    0,  200,  100,\n",
       "           0,    0,  200]),\n",
       " array([0.00010558, 0.00115245, 0.00219933, 0.0032462 , 0.00429307,\n",
       "        0.00533994, 0.00638681, 0.00743369, 0.00848056, 0.00952743,\n",
       "        0.0105743 , 0.01162117, 0.01266805, 0.01371492, 0.01476179,\n",
       "        0.01580866, 0.01685554, 0.01790241, 0.01894928, 0.01999615,\n",
       "        0.02104302, 0.0220899 , 0.02313677, 0.02418364, 0.02523051,\n",
       "        0.02627738, 0.02732426, 0.02837113, 0.029418  , 0.03046487,\n",
       "        0.03151175, 0.03255862, 0.03360549, 0.03465236, 0.03569923,\n",
       "        0.03674611, 0.03779298, 0.03883985, 0.03988672, 0.04093359,\n",
       "        0.04198047, 0.04302734, 0.04407421, 0.04512108, 0.04616796,\n",
       "        0.04721483, 0.0482617 , 0.04930857]))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(cls_pred, bins=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00010882, 0.00010766, 0.00010967, 0.00011892, 0.00017907,\n",
       "       0.00058227, 0.00027306, 0.00225461, 0.00921277, 0.01559986,\n",
       "       0.02904316, 0.03486495, 0.04139331, 0.04460635, 0.04930857,\n",
       "       0.04576903, 0.04855154, 0.04447547, 0.03993549, 0.03767362,\n",
       "       0.03681387, 0.03882298, 0.03758044, 0.03529901, 0.03655646,\n",
       "       0.03302651, 0.03270813, 0.03242896, 0.02748034, 0.02887747,\n",
       "       0.02697163, 0.02679262, 0.02720231, 0.02486099, 0.02395836,\n",
       "       0.02343223, 0.01984066, 0.01332955, 0.00698964, 0.00250044,\n",
       "       0.00041684, 0.00013209, 0.00010795, 0.00010967, 0.00010558,\n",
       "       0.00010859, 0.01937848], dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(cls_pred, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax_1:0' shape=(100,) dtype=int64>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n",
      " 14 14 14 14]\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        print (sess.run(y_pred_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
